{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents\n",
    "By themselves, language models can't take actions - they just output text. A big use case for LangChain is creating agents. \n",
    "Agents are systems that use an LLM as a reasoning enginer to determine which actions to take and what the inputs to those actions should be. \n",
    "The results of those actions can then be fed back into the agent and it determine whether more actions are needed, or whether it is okay to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant functionality\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Load environment variables (set OPENAI_API_KEY and OPENAI_API_BASE in .env)\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Azure OpenAI Service API\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = os.getenv(\"AZURE_OPENAI_VERSION\")\n",
    "openai.api_base = os.getenv('OPENAI_API_ENDPOINT')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Azure OpenAI model\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt4o\",  # replace with your deployment name\n",
    "    temperature=0.5,\n",
    "    max_tokens=100,\n",
    "    openai_api_version=\"2023-03-15-preview\"\n",
    ")\n",
    "\n",
    "# Create the search tool\n",
    "search = TavilySearchResults(max_results=2)\n",
    "tools = [search]\n",
    "\n",
    "# Create the agent with memory\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "agent_executor = create_react_agent(llm, tools, checkpointer=memory)\n",
    "\n",
    "# Configuration for conversation thread\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content=\"I don't have the capability to directly load files or interact with local file systems. However, I can certainly help you with the code to load the `titanic.csv` file into a pandas DataFrame. Here is an example of how you can do it:\\n\\n```python\\nimport pandas as pd\\n\\n# Load the Titanic dataset\\ndf = pd.read_csv('path_to_your_file/titanic.csv')\\n\\n# Display the first few rows of the dataframe\\nprint(df.head())\\n```\\n\\n\", response_metadata={'token_usage': {'completion_tokens': 100, 'prompt_tokens': 88, 'total_tokens': 188}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_5f4bad809a', 'finish_reason': 'length', 'logprobs': None, 'content_filter_results': {}}, id='run-c4b51a7c-2c9a-40fe-ad18-3bd0c2cf94a9-0', usage_metadata={'input_tokens': 88, 'output_tokens': 100, 'total_tokens': 188})]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content=\"To find out the number of rows in the `titanic.csv` file, you can use the following Python code:\\n\\n```python\\nimport pandas as pd\\n\\n# Load the Titanic dataset\\ndf = pd.read_csv('path_to_your_file/titanic.csv')\\n\\n# Get the number of rows\\nnum_rows = df.shape[0]\\n\\nprint(f'The number of rows in the Titanic dataset is: {num_rows}')\\n```\\n\\nThis code will load the CSV file into a pandas\", response_metadata={'token_usage': {'completion_tokens': 100, 'prompt_tokens': 206, 'total_tokens': 306}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_5f4bad809a', 'finish_reason': 'length', 'logprobs': None, 'content_filter_results': {}}, id='run-5ece8f80-19b4-4e4e-a19d-2dba2e36100a-0', usage_metadata={'input_tokens': 206, 'output_tokens': 100, 'total_tokens': 306})]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='The Titanic dataset is one of the most famous datasets used for data analysis and machine learning. It contains information about the passengers aboard the Titanic, which sank on its maiden voyage in 1912. Here are some of the primary features (columns) typically found in the Titanic dataset:\\n\\n1. **PassengerId**: Unique identifier for each passenger.\\n2. **Survived**: Indicates whether the passenger survived (1) or not (0).\\n3. **Pclass**: Passenger class', response_metadata={'token_usage': {'completion_tokens': 100, 'prompt_tokens': 319, 'total_tokens': 419}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_5f4bad809a', 'finish_reason': 'length', 'logprobs': None, 'content_filter_results': {}}, id='run-329494b9-0265-440d-82fc-644be3682c7d-0', usage_metadata={'input_tokens': 319, 'output_tokens': 100, 'total_tokens': 419})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# testing the agent with a few messages\n",
    "\n",
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"load the titanic.csv into a pandas dataframe\")]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")\n",
    "    \n",
    "    \n",
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"how many rows are there in the titanic.csv?\")]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")\n",
    "    \n",
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"tell me more about the passengers\")]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Threads.create() got an unexpected keyword argument 'assistant_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Interact with a specific assistant\u001b[39;00m\n\u001b[0;32m     38\u001b[0m assistant_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour_assistant_id\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 39\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43minteract_with_assistant\u001b[49m\u001b[43m(\u001b[49m\u001b[43massistant_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms the weather in San Francisco?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[1;32mIn[8], line 19\u001b[0m, in \u001b[0;36minteract_with_assistant\u001b[1;34m(assistant_id, message)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minteract_with_assistant\u001b[39m(assistant_id, message):\n\u001b[1;32m---> 19\u001b[0m     thread \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43massistant_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massistant_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     client\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39mthreads\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     21\u001b[0m         thread_id\u001b[38;5;241m=\u001b[39mthread\u001b[38;5;241m.\u001b[39mid,\n\u001b[0;32m     22\u001b[0m         content\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: message}}]\n\u001b[0;32m     23\u001b[0m     )\n\u001b[0;32m     24\u001b[0m     run \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39mthreads\u001b[38;5;241m.\u001b[39mruns\u001b[38;5;241m.\u001b[39mcreate(thread_id\u001b[38;5;241m=\u001b[39mthread\u001b[38;5;241m.\u001b[39mid)\n",
      "\u001b[1;31mTypeError\u001b[0m: Threads.create() got an unexpected keyword argument 'assistant_id'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basiclc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
