{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step #1 Azure OpenAI Service - Chat on private data using LangChain\n",
    "\n",
    "Import note this tutorial is for langchain 0.2\n",
    "The functions below might not work with other langchain versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# Load environment variables (set OPENAI_API_KEY and OPENAI_API_BASE in .env)\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Azure OpenAI Service API\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = os.getenv(\"AZURE_OPENAI_VERSION\")\n",
    "openai.api_base = os.getenv('OPENAI_API_ENDPOINT')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step #2 Define an LLM Config\n",
    "First, we will simply invoke the LLM with a basic prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's one for you:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n"
     ]
    }
   ],
   "source": [
    "# Init LLM and embeddings model\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt4o\", \n",
    "                      temperature=0.5,\n",
    "                      max_tokens=100, \n",
    "                      openai_api_version=\"2023-03-15-preview\")\n",
    "\n",
    "response = llm.invoke(\"Tell me a joke\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config LLM\n",
    "Next we will determine the behavior of our llms configs. This allows us to reuse them for different purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Your Name]  \n",
      "[Your Address]  \n",
      "[City, State, ZIP Code]  \n",
      "[Email Address]  \n",
      "[Phone Number]  \n",
      "[Date]\n",
      "\n",
      "[Recipient's Name]  \n",
      "[Job Title]  \n",
      "[Company Name]  \n",
      "[Company Address]  \n",
      "[City, State, ZIP Code]\n",
      "\n",
      "Dear [Recipient's Name],\n",
      "\n",
      "I am writing to express my enthusiastic interest in the Software Engineer position at [Company Name], as advertised on [where you found the job posting]. With a strong foundation in computer science, a passion for innovative problem-solving, and hands-on experience in software development, I am excited about the opportunity to contribute to your team and help drive the next wave of technological advancements at [Company Name].\n",
      "\n",
      "I hold a [Your Degree] in Computer Science from [Your University], where I developed a robust understanding of algorithms, data structures, and software design principles. My academic background is complemented by professional experience at [Previous Company Name], where I worked as a [Your Previous Job Title]. In this role, I was responsible for designing, developing, and maintaining scalable software solutions that significantly improved user experience and operational efficiency.\n",
      "\n",
      "Some of my key accomplishments include:\n",
      "- Leading a team to develop a customer relationship management (CRM) system that reduced client response time by 40%.\n",
      "- Implementing a machine learning algorithm that enhanced the accuracy of our predictive analytics by 30%.\n",
      "- Collaborating with cross-functional teams to integrate new features into existing platforms, ensuring seamless user experiences and robust performance.\n",
      "\n",
      "My\n"
     ]
    }
   ],
   "source": [
    "# Define configuration with system instructions\n",
    "config_writer = {\n",
    "    \"deployment_name\": \"gpt4o\",\n",
    "    \"temperature\": 0.6,\n",
    "    \"max_tokens\": 300,\n",
    "    \"openai_api_version\": \"2023-03-15-preview\",\n",
    "    \"instruction\": \"You are a creative writing assistant.\"\n",
    "}\n",
    "\n",
    "# Init LLM model\n",
    "llm_writer = AzureChatOpenAI(deployment_name=config_writer[\"deployment_name\"], \n",
    "                      temperature=config_writer[\"temperature\"],\n",
    "                      max_tokens=config_writer[\"max_tokens\"], \n",
    "                      openai_api_version=config_writer[\"openai_api_version\"])\n",
    "\n",
    "# Add system instruction to the prompt\n",
    "user_prompt = 'Write an application letter for a job as a software engineer.'\n",
    "response = llm_writer.invoke(f'{config_writer[\"instruction\"]} ' + user_prompt)\n",
    "content = response.content\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letter continues below:\n",
      "\n",
      "technical proficiency spans multiple programming languages, including Java, Python, and C++, as well as experience with frameworks such as Django and Spring. I am also well-versed in agile methodologies and have a proven track record of delivering high-quality software solutions on time and within budget.\n",
      "\n",
      "What excites me most about the Software Engineer position at [Company Name] is the opportunity to work on cutting-edge projects that push the boundaries of technology. I am particularly impressed by [specific project or aspect of the company that excites you], and I am eager to bring my skills and experience to your team to help achieve the company's goals.\n",
      "\n",
      "I am confident that my background and passion for software engineering make me a strong candidate for this position. I look forward to the opportunity to discuss how my skills and experiences align with the needs of your team. Thank you for considering my application. I hope to hear from you soon to arrange an interview.\n",
      "\n",
      "Sincerely,\n",
      "\n",
      "[Your Name]\n",
      "\n",
      "---\n",
      "\n",
      "### Review and Suggestions for Improvement:\n",
      "\n",
      "1. **Personalization**: The letter is well-structured, but it could benefit from a bit more personalization. Try to include more specific details about the company and the role that genuinely excite you. This will show that you have done your homework and are truly interested in this particular position.\n",
      "\n",
      "2. **Quantifiable Achievements**: While you have mentioned some impressive achievements, adding more quantifiable metrics can make your accomplishments stand out even more. For example, if you can specify the number of users impacted by your CRM system or the scale of the project, it would add more weight to your claims.\n",
      "\n",
      "3. **Technical Skills**: You have listed your technical skills, which is great. However, consider adding a brief example of a project where you applied these skills successfully. This will give the reader a concrete idea of how you can contribute to their team.\n",
      "\n",
      "4. **Soft Skills**: While technical skills are crucial, don't forget to mention soft skills such as teamwork, communication, and problem-solving. These are equally important and can make you a more well-rounded candidate.\n",
      "\n",
      "5. **Closing Statement**: The closing statement is polite and professional, but it could be a bit more assertive. Instead of \"I hope to hear from you soon to arrange an interview,\" you might say, \"I am looking forward to the opportunity to discuss how my background, skills, and enthusiasm can contribute to the success of [Company Name].\"\n",
      "\n",
      "6. **Proofreading**: Ensure\n"
     ]
    }
   ],
   "source": [
    "config_reviewer = {\n",
    "    \"deployment_name\": \"gpt4o\",\n",
    "    \"temperature\": 0.4,\n",
    "    \"max_tokens\": 500,\n",
    "    \"openai_api_version\": \"2023-03-15-preview\",\n",
    "    \"instruction\": \"You are a critical reviewer, who's an expert in reviewing others content and providing helpful advice for improvements.\" \n",
    "}\n",
    "\n",
    "# Init LLM model\n",
    "llm_params = {key: config_reviewer[key] for key in config_reviewer if key != \"instruction\"}  # Exclude 'instruction'\n",
    "llm_reviewer = AzureChatOpenAI(**llm_params)\n",
    "\n",
    "user_prompt = 'Review the following content: ' + content\n",
    "response = llm_reviewer.invoke(f'{config_reviewer[\"instruction\"]} ' + user_prompt)\n",
    "content = response.content\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step #3 Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The core idea of agents is to use a language model to choose a sequence of actions to take. \n",
    "# In chains, a sequence of actions is hardcoded (in code). \n",
    "# In agents, a language model is used as a reasoning engine to determine which actions to take and in which order.\n",
    "# More information: https://python.langchain.com/v0.1/docs/modules/agents/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SimpleChain' from 'langchain.chains' (c:\\Users\\ffollonier\\AppData\\Local\\anaconda3\\envs\\basiclc\\Lib\\site-packages\\langchain\\chains\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenLLM\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleChain\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Memory\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Agent\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'SimpleChain' from 'langchain.chains' (c:\\Users\\ffollonier\\AppData\\Local\\anaconda3\\envs\\basiclc\\Lib\\site-packages\\langchain\\chains\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import OpenLLM\n",
    "from langchain.chains import SimpleChain\n",
    "from langchain.memory import Memory\n",
    "from langchain.agents import Agent\n",
    "\n",
    "# Load configuration from file\n",
    "config = LangChainConfig.from_file(\"langchain.yaml\")\n",
    "\n",
    "# Initialize components based on the configuration\n",
    "memory = Memory.from_config(config.memory)\n",
    "simple_chain = SimpleChain.from_config(config.chains['simple_chain'])\n",
    "llm = OpenLLM(\n",
    "    model_name=config.model_name,\n",
    "    model_id=config.model_id,\n",
    "    server_url=config.server_url if hasattr(config, 'server_url') else None\n",
    ")\n",
    "agent = Agent.from_config(config.agents['default_agent'], memory, llm=llm)\n",
    "\n",
    "# Run the agent\n",
    "agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "loader = DirectoryLoader('../data/qna/', glob=\"*.txt\", loader_cls=TextLoader)\n",
    "\n",
    "documents = loader.load()\n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's ingest them into FAISS so we can efficiently query our embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "db = FAISS.from_documents(documents=docs, embedding=embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a chain that can do the whole chat on our embedding database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Adapt if needed\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\")\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm,\n",
    "                                           retriever=db.as_retriever(),\n",
    "                                           condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "                                           return_source_documents=True,\n",
    "                                           verbose=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's ask a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure OpenAI is a service that provides REST API access to OpenAI's language models, including GPT-3, Codex, and Embeddings model series. These models can be used for content generation, summarization, semantic search, and natural language to code translation. Users can access the service through REST APIs, Python SDK, or a web-based interface in the Azure OpenAI Studio. The service is available in East US, South Central US, and West Europe regions. Additionally, Azure OpenAI offers private networking, regional availability, and responsible AI content filtering.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"what is azure openai service?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to easy implement chat conversations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what is Azure OpenAI Service?\n",
      "Answer: Azure OpenAI Service is a cloud-based service that provides REST API access to OpenAI's language models, including GPT-3, Codex, and Embeddings model series. These models can be used for various tasks such as content generation, summarization, semantic search, and natural language to code translation. Users can access the service through REST APIs, Python SDK, or a web-based interface in the Azure OpenAI Studio. The service is built on Microsoft Azure and offers features such as virtual network support, managed identity, and responsible AI content filtering. However, access to the service is currently limited due to high demand and Microsoft's commitment to responsible AI.\n",
      "Question: Which regions does the service support?\n",
      "Answer: The Azure OpenAI Service is currently available in the East US, South Central US, and West Europe regions.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"what is Azure OpenAI Service?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", result[\"answer\"])\n",
    "\n",
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = \"Which regions does the service support?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-qna-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4ee1bbf3137c7ea9420c4fd488a55642063e5739fe2a7286130d9ba47405b69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
