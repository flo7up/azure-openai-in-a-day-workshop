{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents\n",
    "By themselves, language models can't take actions - they just output text. A big use case for LangChain is creating agents. \n",
    "Agents are systems that use an LLM as a reasoning enginer to determine which actions to take and what the inputs to those actions should be. \n",
    "The results of those actions can then be fed back into the agent and it determine whether more actions are needed, or whether it is okay to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant functionality\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Load environment variables (set OPENAI_API_KEY and OPENAI_API_BASE in .env)\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Azure OpenAI Service API\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = os.getenv(\"AZURE_OPENAI_VERSION\")\n",
    "openai.api_base = os.getenv('OPENAI_API_ENDPOINT')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Azure OpenAI model\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt4o\",  # replace with your deployment name\n",
    "    temperature=0.5,\n",
    "    max_tokens=100,\n",
    "    openai_api_version=\"2023-03-15-preview\"\n",
    ")\n",
    "\n",
    "# Create the search tool\n",
    "search = TavilySearchResults(max_results=2)\n",
    "tools = [search]\n",
    "\n",
    "# Create the agent with memory\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "agent_executor = create_react_agent(llm, tools, checkpointer=memory)\n",
    "\n",
    "# Configuration for conversation thread\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content=\"Sure, I can help you with that. However, I don't have direct access to files like `titanic.csv`. If you provide the contents of the file or a link to it, I can guide you on how to load it into a pandas DataFrame. Here's a general example of how you can load a CSV file into a pandas DataFrame:\\n\\n```python\\nimport pandas as pd\\n\\n# Load the CSV file into a DataFrame\\ndf = pd.read_csv('path/to/t\", response_metadata={'token_usage': {'completion_tokens': 100, 'prompt_tokens': 88, 'total_tokens': 188}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_5f4bad809a', 'finish_reason': 'length', 'logprobs': None, 'content_filter_results': {}}, id='run-028e9436-8049-483d-a6eb-1f9f6701de61-0', usage_metadata={'input_tokens': 88, 'output_tokens': 100, 'total_tokens': 188})]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content=\"To determine the number of rows in the `titanic.csv` file, you would typically load the file into a pandas DataFrame and then use the `shape` attribute to get the number of rows. Here's how you can do it:\\n\\n```python\\nimport pandas as pd\\n\\n# Load the CSV file into a DataFrame\\ndf = pd.read_csv('path/to/titanic.csv')\\n\\n# Get the number of rows\\nnum_rows = df.shape[0]\\nprint(f'The\", response_metadata={'token_usage': {'completion_tokens': 100, 'prompt_tokens': 206, 'total_tokens': 306}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_5f4bad809a', 'finish_reason': 'length', 'logprobs': None, 'content_filter_results': {}}, id='run-aa658867-a9c2-43f1-bc1d-ed17d184ca33-0', usage_metadata={'input_tokens': 206, 'output_tokens': 100, 'total_tokens': 306})]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='To provide more information about the passengers in the Titanic dataset, we typically analyze the DataFrame after loading the CSV file. Here are some common steps and types of information you might want to extract:\\n\\n1. **Basic Information**: \\n   - Number of rows (passengers) and columns (features).\\n   - Column names and data types.\\n   - Basic statistics for numerical columns.\\n\\n2. **Passenger Demographics**:\\n   - Distribution of passengers by gender.\\n   - Distribution of passengers by', response_metadata={'token_usage': {'completion_tokens': 100, 'prompt_tokens': 319, 'total_tokens': 419}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_5f4bad809a', 'finish_reason': 'length', 'logprobs': None, 'content_filter_results': {}}, id='run-017b86ce-a6da-4e40-a079-4ba1b1ac9acf-0', usage_metadata={'input_tokens': 319, 'output_tokens': 100, 'total_tokens': 419})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# testing the agent with a few messages\n",
    "\n",
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"load the titanic.csv into a pandas dataframe\")]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")\n",
    "    \n",
    "    \n",
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"how many rows are there in the titanic.csv?\")]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")\n",
    "    \n",
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"tell me more about the passengers\")]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Threads.create() got an unexpected keyword argument 'assistant_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Interact with a specific assistant\u001b[39;00m\n\u001b[0;32m     38\u001b[0m assistant_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour_assistant_id\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 39\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43minteract_with_assistant\u001b[49m\u001b[43m(\u001b[49m\u001b[43massistant_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms the weather in San Francisco?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[1;32mIn[8], line 19\u001b[0m, in \u001b[0;36minteract_with_assistant\u001b[1;34m(assistant_id, message)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minteract_with_assistant\u001b[39m(assistant_id, message):\n\u001b[1;32m---> 19\u001b[0m     thread \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43massistant_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massistant_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     client\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39mthreads\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     21\u001b[0m         thread_id\u001b[38;5;241m=\u001b[39mthread\u001b[38;5;241m.\u001b[39mid,\n\u001b[0;32m     22\u001b[0m         content\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: message}}]\n\u001b[0;32m     23\u001b[0m     )\n\u001b[0;32m     24\u001b[0m     run \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39mthreads\u001b[38;5;241m.\u001b[39mruns\u001b[38;5;241m.\u001b[39mcreate(thread_id\u001b[38;5;241m=\u001b[39mthread\u001b[38;5;241m.\u001b[39mid)\n",
      "\u001b[1;31mTypeError\u001b[0m: Threads.create() got an unexpected keyword argument 'assistant_id'"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Initialize the client\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-05-01-preview\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# Example function to list assistants\n",
    "def list_assistants():\n",
    "    response = client.beta.assistants.list(order=\"desc\", limit=20)\n",
    "    return response.data\n",
    "\n",
    "# Example function to interact with an assistant\n",
    "def interact_with_assistant(assistant_id, message):\n",
    "    thread = client.beta.threads.create(assistant_id=assistant_id)\n",
    "    client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        content=[{\"type\": \"text\", \"text\": {\"value\": message}}]\n",
    "    )\n",
    "    run = client.beta.threads.runs.create(thread_id=thread.id)\n",
    "    \n",
    "    while run.status not in [\"completed\", \"cancelled\", \"expired\", \"failed\"]:\n",
    "        time.sleep(5)\n",
    "        run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
    "    \n",
    "    messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "    return messages\n",
    "\n",
    "# List assistants\n",
    "assistants = list_assistants()\n",
    "print(assistants)\n",
    "\n",
    "# Interact with a specific assistant\n",
    "assistant_id = \"your_assistant_id\"\n",
    "response = interact_with_assistant(assistant_id, \"What's the weather in San Francisco?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basiclc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
